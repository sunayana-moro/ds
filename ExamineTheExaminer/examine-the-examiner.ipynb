{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12278202,"sourceType":"datasetVersion","datasetId":1819}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Examine The Examiner: Algorithmic Clickbait Headlines Analysis\n\n## Overview\n\nThis notebook explores the fascinating dataset of over 3 million article headlines from *The Examiner*, a pseudo news site that produced an enormous volume of click-driven, algorithmic-style headlines between 2010 and 2015.\n\n## Contents\n\n- Data loading and preprocessing  \n- SQL-based analysis of publication trends  \n- Word frequency and headline pattern exploration  \n- Visualization of monthly and yearly headline dynamics  \n- Insights about the rise and fall of clickbait-driven digital journalism  \n\n## Objective\n\nThe goal is to uncover how *The Examiner* crafted viral headlines and shaped trends, offering a glimpse into early algorithmic media before the AI era.\n\n---\n\n*Let’s dive in to discover what lessons this massive archive holds about digital content, media evolution, and the economics of attention!*\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport sqlite3\n\n# Load dataset CSV and rceate SQLite connection\ndf = pd.read_csv('../input/examine-the-examiner/examiner-date-text.csv')\nconn = sqlite3.connect('examiner_headlines.db')\n\n# Uploading dataframe to SQLite\ndf.to_sql('headlines', conn, index=False, if_exists='replace')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:26:27.942242Z","iopub.execute_input":"2025-09-18T16:26:27.942621Z","iopub.status.idle":"2025-09-18T16:26:40.063929Z","shell.execute_reply.started":"2025-09-18T16:26:27.942581Z","shell.execute_reply":"2025-09-18T16:26:40.062997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Articles published every year","metadata":{}},{"cell_type":"code","source":"query= '''SELECT \n  SUBSTR(publish_date, 1, 4) AS year,\n  COUNT(*) AS articles_count\nFROM headlines\nGROUP BY year\nORDER BY year;'''\nresult = pd.read_sql(query, conn)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:32:42.179140Z","iopub.execute_input":"2025-09-18T16:32:42.179479Z","iopub.status.idle":"2025-09-18T16:32:44.231114Z","shell.execute_reply.started":"2025-09-18T16:32:42.179444Z","shell.execute_reply":"2025-09-18T16:32:44.230186Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SQL Query for the Top 20 Most Frequent Words in Headlines","metadata":{}},{"cell_type":"code","source":"# Ensure all entries are strings and fill NaN values with empty string and flatten list of words, exclude empty strings\nwords = df['headline_text'].fillna('').astype(str).str.lower().str.split(' ')\n\nflat_words = pd.DataFrame({'word': [w for ws in words for w in ws if w.strip() != '']})\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:38:07.701223Z","iopub.execute_input":"2025-09-18T16:38:07.701616Z","iopub.status.idle":"2025-09-18T16:38:25.911792Z","shell.execute_reply.started":"2025-09-18T16:38:07.701586Z","shell.execute_reply":"2025-09-18T16:38:25.910898Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Why Python Preprocessing Is Necessary**\n\nRaw text data in fields like headline_text is often unstructured, noisy, and inconsistent, containing issues such as missing values, upper/lowercase variation, punctuation, and non-standard content. Most SQL engines including SQLite in Kaggle,do not natively support advanced string splitting or tokenization, making tasks like word frequency and keyword extraction impractical or error-prone in pure SQL","metadata":{}},{"cell_type":"code","source":"flat_words.to_sql('headline_words', conn, index=False, if_exists='replace')\n\nquery = '''\nSELECT word, COUNT(*) AS freq\nFROM headline_words\nGROUP BY word\nORDER BY freq DESC\nLIMIT 20;\n'''\nresult = pd.read_sql(query, conn)\nprint(result)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:38:33.373159Z","iopub.execute_input":"2025-09-18T16:38:33.373876Z","iopub.status.idle":"2025-09-18T16:39:40.705352Z","shell.execute_reply.started":"2025-09-18T16:38:33.373846Z","shell.execute_reply":"2025-09-18T16:39:40.703855Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Query to count only words that have at least 3 characters, excluding shorter words like “a”, “to”, “in” etc.","metadata":{}},{"cell_type":"code","source":"query = '''\nSELECT word, COUNT(*) AS freq\nFROM headline_words\nWHERE LENGTH(word) >= 3\nGROUP BY word\nORDER BY freq DESC\nLIMIT 20;\n'''\nresult = pd.read_sql(query, conn)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:42:23.089255Z","iopub.execute_input":"2025-09-18T16:42:23.089760Z","iopub.status.idle":"2025-09-18T16:42:48.018174Z","shell.execute_reply.started":"2025-09-18T16:42:23.089709Z","shell.execute_reply":"2025-09-18T16:42:48.017036Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query = '''\nSELECT word, COUNT(*) AS freq\nFROM headline_words\nWHERE LENGTH(word) >= 4\nGROUP BY word\nORDER BY freq DESC\nLIMIT 20;\n'''\nresult = pd.read_sql(query, conn)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:51:35.194517Z","iopub.execute_input":"2025-09-18T16:51:35.194861Z","iopub.status.idle":"2025-09-18T16:51:57.359845Z","shell.execute_reply.started":"2025-09-18T16:51:35.194837Z","shell.execute_reply":"2025-09-18T16:51:57.358733Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Top Bigrams and Trigrams in Headlines\n\nThis section identifies the most frequent phrases composed of two and three words (bigrams and trigrams) found in the headlines dataset. These multi-word phrases often capture common news topics, recurring themes, and characteristic clickbait constructions.\n\n### Methodology\n\n- Headlines are first tokenized and split into consecutive word pairs (bigrams) and triplets (trigrams) using Python preprocessing.\n- These phrase lists are then loaded into SQLite tables for efficient frequency counting.\n- The above SQL queries extract the top 20 most frequent bigrams and trigrams, revealing key word combinations driving headline trends.\n\n### SQL Queries\n\n","metadata":{}},{"cell_type":"code","source":"from collections import Counter\nimport pandas as pd\n\n# Clean and tokenize headlines into words \ntokens = df['headline_text'].fillna('').astype(str).str.lower().str.split()\n\n# Extract bigrams (2-word phrases)\nbigrams = []\nfor headline_words in tokens:\n    bigrams.extend([' '.join(headline_words[i:i+2]) for i in range(len(headline_words)-1)])\n\n# Extract trigrams (3-word phrases)\ntrigrams = []\nfor headline_words in tokens:\n    trigrams.extend([' '.join(headline_words[i:i+3]) for i in range(len(headline_words)-2)])\n# Extract quadgrams (4-word phrases)\nquadgrams = []\nfor headline_words in tokens:\n    quadgrams.extend([' '.join(headline_words[i:i+4]) for i in range(len(headline_words)-2)])\n\nbigrams_df = pd.DataFrame({'phrase': bigrams})\ntrigrams_df = pd.DataFrame({'phrase': trigrams})\nquadgrams_df = pd.DataFrame({'phrase': quadgrams})\n\n\nbigrams_df.to_sql('bigrams', conn, index=False, if_exists='replace')\ntrigrams_df.to_sql('trigrams', conn, index=False, if_exists='replace')\nquadgrams_df.to_sql('quadgrams', conn, index=False, if_exists='replace')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T18:15:43.995115Z","iopub.execute_input":"2025-09-18T18:15:43.995455Z","iopub.status.idle":"2025-09-18T18:18:55.599818Z","shell.execute_reply.started":"2025-09-18T18:15:43.995420Z","shell.execute_reply":"2025-09-18T18:18:55.598891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"top_bigrams_query = '''\nSELECT phrase, COUNT(*) AS freq\nFROM bigrams\nGROUP BY phrase\nORDER BY freq DESC\nLIMIT 20;\n'''\n\ntop_trigrams_query = '''\nSELECT phrase, COUNT(*) AS freq\nFROM trigrams\nGROUP BY phrase\nORDER BY freq DESC\nLIMIT 20;\n'''\ntop_quadgrams_query = '''\nSELECT phrase, COUNT(*) AS freq\nFROM quadgrams\nGROUP BY phrase\nORDER BY freq DESC\nLIMIT 20;\n'''\n\ntop_bigrams = pd.read_sql(top_bigrams_query, conn)\ntop_trigrams = pd.read_sql(top_trigrams_query, conn)\ntop_quadgrams = pd.read_sql(top_quadgrams_query, conn)\n\nprint(\"Top 20 bigrams:\")\nprint(top_bigrams)\nprint(\"\\nTop 20 trigrams:\")\nprint(top_trigrams)\nprint(\"\\nTop 20 quadgrams:\")\nprint(top_quadgrams)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T18:27:04.275370Z","iopub.execute_input":"2025-09-18T18:27:04.275744Z","iopub.status.idle":"2025-09-18T18:28:28.306462Z","shell.execute_reply.started":"2025-09-18T18:27:04.275713Z","shell.execute_reply":"2025-09-18T18:28:28.305430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Analysing headline length distribution","metadata":{}},{"cell_type":"code","source":"query = '''\nSELECT \n  SUBSTR(publish_date, 1, 6) AS year_month,\n  COUNT(*) AS articles_count\nFROM headlines\nGROUP BY year_month\nORDER BY year_month;\n'''\nresult = pd.read_sql(query, conn)\nprint(result)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:47:16.173444Z","iopub.execute_input":"2025-09-18T16:47:16.173815Z","iopub.status.idle":"2025-09-18T16:47:18.333016Z","shell.execute_reply.started":"2025-09-18T16:47:16.173787Z","shell.execute_reply":"2025-09-18T16:47:18.332042Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Example: yearly article counts\nyearly_counts = pd.read_sql('''\nSELECT SUBSTR(publish_date, 1, 4) AS year, COUNT(*) AS count \nFROM headlines GROUP BY year ORDER BY year;\n''', conn)\n\nplt.figure(figsize=(10,6))\nplt.plot(yearly_counts['year'], yearly_counts['count'], marker='o')\nplt.title('Articles Published Per Year')\nplt.xlabel('Year')\nplt.ylabel('Number of Articles')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T16:48:09.212917Z","iopub.execute_input":"2025-09-18T16:48:09.213281Z","iopub.status.idle":"2025-09-18T16:48:11.655038Z","shell.execute_reply.started":"2025-09-18T16:48:09.213256Z","shell.execute_reply":"2025-09-18T16:48:11.653995Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Monthly distribution of articles published each year from 2010 to 2015.\n\n* Each dashed line represents the article count for a specific year, showing trends and seasonality within that year.\n* The solid red line indicates the average number of articles published per month across all six years.\n* This helps identify consistent monthly patterns and highlights months with unusually high or low publication volume.\n* For example, peaks in certain months could correspond to major news events or seasonally relevant reporting periods.\n* The visualization combines detailed year-wise data with a clear average reference, supporting trend analysis and comparative exploration of publishing behavior over time.\n","metadata":{}},{"cell_type":"markdown","source":"**Note on Data Exclusion: Years 2009 and 2016**\nYears 2009 and 2016 were excluded from this analysis because they respectively represent the rise and decline phases of The Examiner’s publishing activity. These boundary years may contain incomplete data or exhibit extreme fluctuations, which could act as outliers and skew overall trends.","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Run the SQL query to get the pivoted data with averages\nquery = '''\nWITH monthly_counts AS (\n  SELECT \n    SUBSTR(publish_date, 5, 2) AS month,\n    SUBSTR(publish_date, 1, 4) AS year,\n    COUNT(*) AS articles_count\n  FROM headlines\n  GROUP BY year, month\n),\npivoted AS (\n  SELECT \n    month,\n    SUM(CASE WHEN year = '2010' THEN articles_count ELSE 0 END) AS \"2010\",\n    SUM(CASE WHEN year = '2011' THEN articles_count ELSE 0 END) AS \"2011\",\n    SUM(CASE WHEN year = '2012' THEN articles_count ELSE 0 END) AS \"2012\",\n    SUM(CASE WHEN year = '2013' THEN articles_count ELSE 0 END) AS \"2013\",\n    SUM(CASE WHEN year = '2014' THEN articles_count ELSE 0 END) AS \"2014\",\n    SUM(CASE WHEN year = '2015' THEN articles_count ELSE 0 END) AS \"2015\"\n  FROM monthly_counts\n  GROUP BY month\n)\nSELECT \n  month,\n  \"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\",\n  ROUND( (\"2010\" + \"2011\" + \"2012\" + \"2013\" + \"2014\" + \"2015\") / 6.0, 2) AS average\nFROM pivoted\nORDER BY month;\n'''\n\ndf = pd.read_sql(query, conn)\n\n# Convert month to integer for plotting\ndf['month'] = df['month'].astype(int)\n\nplt.figure(figsize=(12, 6))\n\n# Plot each year\nyears = [\"2010\", \"2011\", \"2012\", \"2013\", \"2014\", \"2015\"]\nfor year in years:\n    plt.plot(df['month'], df[year], label=year, linestyle='--', alpha=0.7)\n\n# Plot average with a distinct color and solid line\nplt.plot(df['month'], df['average'], label='Average', color='red', linewidth=2.5)\n\nplt.title('Monthly Article Counts by Year with Average')\nplt.xlabel('Month')\nplt.ylabel('Number of Articles')\nplt.xticks(range(1, 13))\nplt.legend()\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-18T17:31:05.805289Z","iopub.execute_input":"2025-09-18T17:31:05.805612Z","iopub.status.idle":"2025-09-18T17:31:09.622286Z","shell.execute_reply.started":"2025-09-18T17:31:05.805586Z","shell.execute_reply":"2025-09-18T17:31:09.621271Z"}},"outputs":[],"execution_count":null}]}